:speaking_head: \[ [ä¸­æ–‡](./README.md) | **English** \]

<p align="center">
    <br>
    <img src="./assets/KnowLM.png" width="400" height="120"/>
    <br>
</p>

# Knowledgable Large Language Model Framework.

With the rapid development of deep learning technology, large language models such as ChatGPT have made substantial strides in the realm of natural language processing. However, these expansive models still encounter several challenges in acquiring and comprehending knowledge, including the difficulty of updating knowledge and potential knowledge discrepancies and biases, collectively known as **knowledge fallacies**. The KnowLM project endeavors to tackle these issues by launching an open-source large-scale knowledgable language model framework and releasing corresponding models. 

The project's `initial phase` introduced a knowledge extraction LLM based on LLaMA, dubbed **ZhiXi** (**æ™ºæ**, which means intelligent analysis of data for knowledge extraction). To integrate the capacity of Chinese understanding into the language models without compromising their inherent knowledge, we firstly <b>(1) use Chinese corpora for the full-scale pre-training with LLaMA (13B), augment the language model's understanding of Chinese and improve its knowledge richness while retaining its original English and code capacities;</b> Then <b>(2) we fine-tune the model obtained from the first step with an instruction dataset, thus bolstering the language model's understanding of human instructions for knowledge extraction.</b>
- â—Please note that this project is still undergoing optimization, and the model weights will be regularly updated to support new features and models!

**The features of this project are as follows:**

- Centered on knowledge and large models, a **full-scale pre-training** of the large model, such as LLaMA, is conducted using the built Chinese&English pre-training corpus.
- Based on the technology of **KG2Instructions**, the knowledge extraction tasks, including NER, RE, and IE, are optimized and can be completed using human instructions.
- Using the built Chinese instruction dataset (approximately 1400K), LoRA fine-tuning is used to enhance the model's understanding of human instructions.
- The weights of the pre-training model and LoRA's instruction fine-tuning are open-sourced.
- The **full-scale pre-training code** (providing conversion, construction, and loading of large corpora) and **LoRA instruction fine-tuning code** are open-sourced (support multi-machine multi-GPU).


All weights have been uploaded to HuggingFaceğŸ¤—. It should be noted that all the following effects are based on `ZhiXi-13B-Diff`. If you have downloaded `ZhiXi-13B-Diff-fp16`, there may be some variations in the effects.
| Model Name       | Train Method    | Weight Type          | Size     | Download Link                           | Notes                                                         |
| -------------- | ------------ | --------------------- | -------- | ---------------------------------- | ------------------------------------------------------------ |
| ZhiXi-13B-Diff | Full Pretraining   | Differential Weights | 48GB     | [HuggingFace](https://huggingface.co/zjunlp/zhixi-13b-diff) <br/> [GoogleDrive](https://drive.google.com/drive/folders/1PZDqZNaBJYQYeON1-9aFBtagktEWAtUK?usp=drive_link)| Restoring the pre-trained weights (i.e. **ZhiXi-13B**) needs to match the weights of `LLaMA-13B`, please refer to [here](#2-2) for specific instructions. |
| ZhiXi-13B-Diff-fp16 | Full Pretraining   | Differential Weights(fp16) | 24GB     | [HuggingFace](https://huggingface.co/zjunlp/zhixi-13b-diff-fp16) <br/> [Google Drive](https://drive.google.com/drive/folders/1LYm-HUSSQ5Rl8nqZcswdiSpcP9xYTXaO?usp=sharing) | The main difference with `ZhiXi-13B-Diff` is the adoption of the `fp16` format for storage, which reduces memory usage. However, it may result in slight differences in the weights obtained from our actual training, which can slightly impact performance. For specific usage instructions, please refer to [here](#2-2) for specific instructions. |
| ZhiXi-13B-LoRA | LoRA Instruction-tuning | LoRA Weights              | 251MB    | [HuggingFace](https://huggingface.co/zjunlp/zhixi-13b-lora) <br/>  [GoogleDrive](https://drive.google.com/drive/folders/1GLyaWIyDIayudrQhb_tJYoNPAUk1xByS?usp=drive_link) | It needs to be used with **ZhiXi-13B**. For specific instructions, please refer to [here](#2-4).          |
| ZhiXi-7B Series   | Coming soon     | Coming soon            | Coming soon | Coming soon                           | Coming soon                                                 |


## NEWS
- \[**June 2023**\] The project name has been changed from `CaMA` to `KnowLM`.
- \[**June 2023**\] Release the first version of pre-trained weights and the LoRA weights.

## Why it's called ZhiXi (æ™ºæ)?
In Chinese, "Zhi" (æ™º) signifies intelligence, referencing the AI's advanced language understanding capabilities. "Xi" (æ) means to analyze or extract, symbolizing the system's knowledge extraction feature. Together, ZhiXi (æ™ºæ) epitomizes an intelligent system adept at dissecting and garnering knowledge - characteristics that align with our expectations of a highly knowledgeable model.

## Contents

- [Cases](#1)
  - [Pretraining Cases](#1-1)
  - [Information Extraction Cases](#1-2)
  - [General Ability Cases](#1-3)
- [Quick Start](#2)
  - [Environment Configuration](#2-1)
  - [Model Weight(Pretrain and LoRA)](#2-2)
  - [Model Usage Guide](#2-4)
  - [Information Extraction Prompt](#2-5)
  - [Combining ZhiXi-13B-LoRA Weights with ZhiXi-13B Weights](#2-6)
  - [MD5](#2-7)
- [Training Details](#3)
  - [Pertraining data and Pretraining scripts](#3-1)
  - [Instruction data and Instruction-tuning scripts](#3-3)
- [Limitations](#4)
- [TODO List](#5)
- [FAQ](#6)
- [Acknowledgments/Contributors/Citations](#7)



<h2 id="1">1. Cases</h2>

<h3 id="1-1">1.1 Pretraining Cases</h3>

Our pre-trained model has demonstrated certain abilities in instruction following, coding, reasoning, as well as some translation capabilities, without any fine-tuning using instructions. Additionally, it has acquired new knowledge. Below are some of our sample cases. If you wish to reproduce our examples and view detailed decoding configuration, please first [set up the environment](#2-1) and [restore the weights](#2-2), then follow the steps outlined [here](#2-4).

> In the follwing cases, text in **bold** represents the prompt, while non-bold text represents the model's output.
>
> Due to the maximum inference length set to 512, our cases fall into three situations:
>
> 1. Compeleted output. The model generates the termination token EOS and completes the output. We mark this with :white_check_mark:.
> 2. Incomplete output. The output is cut off due to the maximum inference length. We mark this with :eight_spoked_asterisk:.
> 3. Repeated output. We remove repeated content manually and mark it with :arrow_left:.

<details>
  <summary><b>Translation</b></summary>


  - <b>ä½ å¾ˆæ“…é•¿å°†ä¸­æ–‡ç¿»è¯‘æˆè‹±æ–‡ã€‚å°†ä¸‹é¢çš„å¥å­ç¿»è¯‘æˆè‹±æ–‡ï¼šæˆ‘ä»¬ä»Šå¤©å‡†å¤‡å»è¥¿å®‰çœ‹å…µé©¬ä¿‘ã€‚ç­”æ¡ˆï¼š</b>We are going to see the Terracotta Warriors in Xi'an today.:white_check_mark:

- **You are good at translating English into Chinese. Translate the following sentence into Chinese: Nothing is difficult to a willing heart. Answer:** ä»€ä¹ˆéƒ½ä¸éš¾,åªè¦å¿ƒç”˜æƒ…æ„¿ã€‚:white_check_mark:
</details>
  

<details>
  <summary><b>Knowledge</b></summary>


- **ä½ é˜…è¯»è¿‡æç™½çš„æ‰€æœ‰è¯—æ­Œã€‚æç™½çš„ã€Šå°†è¿›é…’ã€‹çš„åŸæ–‡æ˜¯**:å›ä¸è§é»„æ²³ä¹‹æ°´å¤©ä¸Šæ¥,å¥”æµåˆ°æµ·ä¸å¤å›ã€‚ å›ä¸è§é«˜å ‚æ˜é•œæ‚²ç™½å‘,æœå¦‚é’ä¸æš®æˆé›ªã€‚ äººç”Ÿå¾—æ„é¡»å°½æ¬¢,è«ä½¿é‡‘æ¨½ç©ºå¯¹æœˆã€‚ å¤©ç”Ÿæˆ‘æå¿…æœ‰ç”¨,åƒé‡‘æ•£å°½è¿˜å¤æ¥ã€‚ çƒ¹ç¾Šå®°ç‰›ä¸”ä¸ºä¹,ä¼šé¡»ä¸€é¥®ä¸‰ç™¾æ¯ã€‚ å²‘å¤«å­,ä¸¹ä¸˜ç”Ÿ,å°†è¿›é…’,å›è«åœã€‚ ä¸å›æ­Œä¸€æ›²,è¯·å›ä¸ºæˆ‘ä¾§è€³å¬ã€‚ é’Ÿé¼“é¦”ç‰ä½•è¶³è´µ,ä½†æ„¿é•¿é†‰ä¸å¤é†’ã€‚ å¤æ¥åœ£è´¤çš†å¯‚å¯,æƒŸæœ‰é¥®è€…ç•™å…¶åã€‚ é™ˆç‹æ˜”æ—¶å®´å¹³ä¹,æ–—é…’ååƒæ£æ¬¢è°‘ã€‚ ä¸»äººä½•ä¸ºè¨€å°‘é’±,å¾„é¡»æ²½å–å¯¹å›é…Œã€‚ äº”èŠ±é©¬,åƒé‡‘è£˜,å‘¼å„¿å°†å‡ºæ¢ç¾é…’,ä¸å°”åŒé”€ä¸‡å¤æ„ã€‚ å”ä»£å¤§è¯—äººæç™½(701-762),å­—å¤ªç™½,å·é’è²å±…å£«ã€‚ ç¥–ç±é™‡è¥¿æˆçºª(ä»Šç”˜è‚ƒå¤©æ°´),éš‹æœ«å…¶å…ˆäººæµå¯“è¥¿åŸŸç¢å¶(å”æ—¶å±å®‰è¥¿éƒ½æŠ¤åºœ,åœ¨ä»Šå‰å°”å‰æ–¯æ–¯å¦åŒ—éƒ¨æ‰˜å…‹é©¬å…‹é™„è¿‘),æç™½å³ç”Ÿäºç¢å¶ã€‚ å¹¼æ—¶éšçˆ¶è¿å±…ç»µå·æ˜Œéš†(ä»Šå››å·æ±Ÿæ²¹)é’è²ä¹¡:eight_spoked_asterisk:
- **åºŠå‰æ˜æœˆå…‰ï¼Œç–‘æ˜¯åœ°ä¸Šéœœã€‚** ä¸¾å¤´æœ›æ˜æœˆï¼Œä½å¤´æ€æ•…ä¹¡ã€‚ è¿™é¦–è¯—å‡ºè‡ªå”ä»£è¯—äººæç™½çš„ã€Šé™å¤œæ€ã€‹ã€‚ ä½œå“åŸæ–‡ åºŠå‰æ˜æœˆå…‰ï¼Œç–‘æ˜¯åœ°ä¸Šéœœã€‚ ä¸¾å¤´æœ›æ˜æœˆï¼Œä½å¤´æ€æ•…ä¹¡ã€‚  [1] ä½œè€…ç®€ä»‹ æç™½ï¼ˆ701ï½762ï¼‰å­—å¤ªç™½ï¼Œå·é’è²å±…å£«ã€‚ ç¥–ç±é™‡è¥¿æˆçºªï¼ˆä»Šç”˜è‚ƒå¤©æ°´é™„è¿‘ï¼‰ï¼Œå…ˆä¸–äºéš‹æœ«æµå¾™è¥¿åŸŸï¼Œæç™½å³ç”Ÿäºä¸­äºšç¢å¶ï¼ˆä»Šå·´å°”å–€ä»€æ¹–å—é¢çš„æ¥šæ²³æµåŸŸï¼Œå”æ—¶å±å®‰è¥¿éƒ½æŠ¤åºœç®¡è¾–ï¼‰ã€‚ å¹¼æ—¶éšçˆ¶è¿å±…ç»µå·æ˜Œéš†ï¼ˆä»Šå››å·æ±Ÿæ²¹ï¼‰é’è²ä¹¡ã€‚ ä»–ä¸€ç”Ÿç»å¤§éƒ¨åˆ†åœ¨æ¼«æ¸¸ä¸­åº¦è¿‡ã€‚ å…¬å…ƒ742å¹´ï¼ˆå¤©å®å…ƒå¹´ï¼‰ï¼Œå› é“å£«å´ç­ çš„æ¨èï¼Œè¢«å¬è‡³é•¿å®‰ï¼Œä¾›å¥‰ç¿°æ—ã€‚ æ–‡ç« é£é‡‡ï¼ŒååŠ¨ä¸€æ—¶ï¼Œé¢‡ä¸ºå”ç„å®—æ‰€èµè¯†ã€‚ åå› ä¸èƒ½è§å®¹äºæƒè´µï¼Œåœ¨äº¬ä»…ä¸‰å¹´ï¼Œå°±å¼ƒå®˜è€Œå»ï¼Œä»ç„¶ç»§ç»­ä»–é‚£é£˜è¡å››æ–¹çš„æµæµªç”Ÿæ´»ã€‚ å…¬å…ƒ756å¹´ï¼Œå³å®‰å²ä¹‹ä¹±å‘ç”Ÿçš„ç¬¬äºŒå¹´ï¼Œä»–æ„Ÿæ„¤æ—¶è‰°ï¼Œæ›¾å‚åŠ äº†æ°¸ç‹æç’˜çš„å¹•åºœã€‚ ä¸å¹¸ï¼Œæ°¸ç‹ä¸è‚ƒå®—å‘ç”Ÿäº†äº‰å¤ºå¸ä½çš„æ–—äº‰ï¼Œå¤±è´¥ä¹‹:eight_spoked_asterisk:
- **è‡ªç„¶è¯­è¨€å¤„ç†æ˜¯**è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒç ”ç©¶å¦‚ä½•ä»è‡ªç„¶è¯­è¨€ä¸­æŠ½å–å‡ºæœ‰æ„ä¹‰çš„ä¿¡æ¯ã€‚ è‡ªç„¶è¯­è¨€å¤„ç†åŒ…æ‹¬è¯æ³•åˆ†æã€å¥æ³•åˆ†æã€è¯­ä¹‰åˆ†æã€è¯­éŸ³è¯†åˆ«å’Œæœºå™¨ç¿»è¯‘ç­‰ã€‚ è‡ªç„¶è¯­è¨€å¤„ç†çš„ç›®æ ‡æ˜¯ä½¿è®¡ç®—æœºèƒ½å¤Ÿåƒäººç±»ä¸€æ ·ç†è§£è‡ªç„¶è¯­è¨€ã€‚:white_check_mark:
</details>


<details>
  <summary><b>Instruction Following</b></summary>


- <b>ä½ éå¸¸äº†è§£ä¸€äº›å¥åº·ç”Ÿæ´»çš„ä¹ æƒ¯ï¼Œè¯·åˆ—ä¸¾å‡ ä¸ªå¥åº·ç”Ÿæ´»çš„å»ºè®®ï¼š</b>1.æ¯å¤©åšæŒé”»ç‚¼30åˆ†é’Ÿä»¥ä¸Šã€‚  2.ä¸å¸çƒŸï¼Œä¸é…—é…’ã€‚  3.å°‘åƒé«˜è„‚è‚ªé£Ÿç‰©ã€‚  4.å¤šåƒè”¬èœå’Œæ°´æœã€‚  5.ä¿è¯å……è¶³çš„ç¡çœ ã€‚  6.ä¿æŒè‰¯å¥½çš„å¿ƒæƒ…ã€‚  7.å®šæœŸä½“æ£€ã€‚  8.å…»æˆè‰¯å¥½çš„å«ç”Ÿä¹ æƒ¯ã€‚:white_check_mark:

- **æˆ‘çˆ±ä½ çš„è‹±æ–‡æ˜¯ä»€ä¹ˆï¼Ÿ** æˆ‘çˆ±ä½ çš„è‹±æ–‡æ˜¯"i love you"ã€‚:arrow_left:

- **ä½ å¯¹ä¸­å›½çš„å¤§å­¦äº†è§£çš„éå¸¸å¤šã€‚è¯·è¯„ä»·ä¸€ä¸‹æµ™æ±Ÿå¤§å­¦æ˜¯ä¸€æ‰€æ€æ ·çš„å­¦æ ¡ã€‚** ç­”:æµ™æ±Ÿå¤§å­¦æ˜¯ä¸€æ‰€å†å²æ‚ ä¹…ã€å­¦ç§‘é½å…¨ã€å­¦æœ¯å®åŠ›é›„åšã€å¸ˆèµ„åŠ›é‡å¼ºå¤§ã€åŠå­¦ç‰¹è‰²é²œæ˜,åœ¨å›½å†…å¤–å…·æœ‰é‡è¦å½±å“çš„ç»¼åˆæ€§ç ”ç©¶å‹å¤§å­¦ã€‚ æµ™æ±Ÿå¤§å­¦æ˜¯é¦–æ‰¹è¿›å…¥å›½å®¶"211å·¥ç¨‹"å’Œ"985å·¥ç¨‹"å»ºè®¾çš„é«˜æ ¡ä¹‹ä¸€,ä¹Ÿæ˜¯å›½å®¶"111è®¡åˆ’"å’Œ"ç å³°è®¡åˆ’"é‡ç‚¹å»ºè®¾çš„åç‰Œå¤§å­¦ã€‚ æµ™æ±Ÿå¤§å­¦çš„å‰èº«æ˜¯åˆ›å»ºäº1897å¹´çš„æ±‚æ˜¯ä¹¦é™¢,æ˜¯ä¸­å›½äººæœ€æ—©è‡ªå·±åˆ›åŠçš„æ–°å¼é«˜ç­‰å­¦åºœä¹‹ä¸€ã€‚ 1928å¹´,æ±‚æ˜¯ä¹¦é™¢æ”¹åˆ¶ä¸ºå›½ç«‹æµ™æ±Ÿå¤§å­¦ã€‚ 1952å¹´,æµ™æ±Ÿå¤§å­¦æˆä¸ºæ–°ä¸­å›½åˆ›åŠçš„ç¬¬ä¸€æ‰¹å…¨å›½é‡ç‚¹å¤§å­¦ã€‚ 1998å¹´,æµ™æ±Ÿå¤§å­¦ä¸æ­å·å¤§å­¦ã€æµ™æ±Ÿå†œä¸šå¤§å­¦ã€æµ™æ±ŸåŒ»ç§‘å¤§å­¦ã€æµ™æ±Ÿå¸ˆèŒƒå¤§å­¦ã€æµ™æ±Ÿå¸ˆèŒƒå¤§å­¦åŒ»å­¦é™¢åˆå¹¶ç»„å»ºæ–°çš„æµ™æ±Ÿå¤§å­¦ã€‚ æµ™æ±Ÿå¤§å­¦ç°å·²å‘å±•æˆä¸ºä¸€æ‰€åŒ…æ‹¬æ–‡ã€æ³•ã€ç†ã€å·¥ã€å†œã€åŒ»ã€ç®¡ã€è‰ºç­‰å­¦ç§‘é—¨ç±»é½å…¨çš„ç»¼åˆæ€§ç ”ç©¶å‹å¤§å­¦,æ˜¯å›½å®¶"985å·¥ç¨‹"å’Œ"211å·¥ç¨‹"é‡ç‚¹å»ºè®¾çš„å¤§å­¦ä¹‹ä¸€,ä¹Ÿæ˜¯å›½å®¶"111è®¡åˆ’"å’Œ"ç å³°è®¡åˆ’"é‡ç‚¹å»ºè®¾çš„åç‰Œå¤§å­¦ã€‚:white_check_mark:

- **ä½ éå¸¸äº†è§£ä¸­å›½çš„å¤§å­¦ã€‚è¯·ä»‹ç»ä¸€ä¸‹æµ™æ±Ÿå¤§å­¦ã€‚** æµ™æ±Ÿå¤§å­¦æ˜¯ä¸­å›½è‘—åçš„é«˜ç­‰å­¦åºœä¹‹ä¸€,å®ƒçš„å‰èº«æ˜¯åˆ›å»ºäº1897å¹´çš„æ±‚æ˜¯ä¹¦é™¢,1928å¹´æ”¹ä¸ºå›½ç«‹æµ™æ±Ÿå¤§å­¦,1952å¹´å®šåä¸ºæµ™æ±Ÿå¤§å­¦ã€‚ æµ™æ±Ÿå¤§å­¦æ˜¯é¦–æ‰¹è¿›å…¥å›½å®¶"211å·¥ç¨‹"å’Œ"985å·¥ç¨‹"é‡ç‚¹å»ºè®¾çš„é«˜æ ¡ä¹‹ä¸€ã€‚ æµ™æ±Ÿå¤§å­¦æœ‰ç€æ‚ ä¹…çš„å†å²å’Œæ·±åšçš„æ–‡åŒ–åº•è•´,åœ¨ä¸­å›½è¿‘ç°ä»£æ•™è‚²å²ä¸Šå æœ‰é‡è¦åœ°ä½ã€‚ æµ™æ±Ÿå¤§å­¦çš„å‰èº«æ±‚æ˜¯ä¹¦é™¢åˆ›å»ºäº1897å¹´,æ˜¯ä¸­å›½äººæœ€æ—©è‡ªå·±åˆ›åŠçš„æ–°å¼é«˜ç­‰å­¦åºœä¹‹ä¸€ã€‚ 1901å¹´,æ±‚æ˜¯ä¹¦é™¢æ›´åä¸ºæµ™æ±Ÿå¤§å­¦å ‚ã€‚ 1928å¹´,æµ™æ±Ÿå¤§å­¦æ­£å¼å®šåä¸ºå›½ç«‹æµ™æ±Ÿå¤§å­¦ã€‚ 1952å¹´,æµ™æ±Ÿå¤§å­¦æˆä¸ºå…¨å›½é¦–æ‰¹é‡ç‚¹å¤§å­¦ã€‚ 1958å¹´,æµ™æ±Ÿå¤§å­¦æˆä¸ºå…¨å›½é¦–æ‰¹åšå£«ã€ç¡•å£«å­¦ä½æˆäºˆå•ä½ã€‚ 1998å¹´,æµ™æ±Ÿå¤§å­¦æˆä¸ºé¦–æ‰¹è¿›å…¥å›½å®¶"211å·¥ç¨‹"é‡ç‚¹å»ºè®¾çš„é«˜æ ¡ã€‚ 2001å¹´,æµ™æ±Ÿå¤§å­¦æˆä¸ºé¦–æ‰¹è¿›å…¥å›½å®¶"985å·¥ç¨‹"é‡ç‚¹å»ºè®¾çš„é«˜æ ¡ã€‚ 2006å¹´,æµ™æ±Ÿå¤§å­¦æˆä¸ºé¦–æ‰¹è¿›å…¥å›½å®¶"111è®¡åˆ’"é‡ç‚¹å»ºè®¾çš„é«˜æ ¡ã€‚ 2017å¹´,æµ™æ±Ÿå¤§å­¦æˆä¸ºé¦–æ‰¹è¿›å…¥å›½å®¶"åŒä¸€æµ"ä¸–ç•Œä¸€æµå¤§å­¦å’Œä¸€æµå­¦ç§‘å»ºè®¾é«˜æ ¡ã€‚ æµ™æ±Ÿå¤§å­¦æ˜¯ä¸€æ‰€ç ”ç©¶å‹ã€ç»¼åˆæ€§ã€:eight_spoked_asterisk:

- **Can you help me write a formal email to a potential business partner proposing a joint venture? Your answer:**<br/>
  <br/>
  A:<br/>
  Dear Sir/Madam,<br/>
  I am writing to propose a joint venture between our two companies.  I have attached a proposal for your consideration.  Please let me know if you would like to discuss this further.<br/>
   Yours sincerely,:white_check_mark:

- **You are now a doctor. Here are some tips for a healthy life. 1.**  Don't smoke.  2.  Eat a balanced diet.  3.  Exercise regularly.  4.  Drink alcohol only in moderation.  5.  Take care of your teeth.  6.  Get enough sleep.  7.  See your doctor for regular checkups.  8.  Learn to manage stress.  9.  Stay up-to-date on vaccinations.  10.  Wash your hands often.  11.  Avoid secondhand smoke.  12.  Use sunscreen and wear sunglasses.  13.  Know your family medical history.  14.  Be aware of the signs of depression.  15.  Protect your hearing.  16.  Practice safe sex.  17.  If you drink coffee, do so in moderation.  18.  Limit your intake of caffeine.  19.  Don't drive when you're tired.  20.  Don't text and drive.  21.  Don't drink and drive.  22.  Don't ride a motorcycle without a helmet.  23.  Don't swim alone.  24.  Don't dive into water where you can't see the bottom.  25.  Don't swim if you've been drinking alcohol.  26.  Don't swim if you're sick.  27.  Don't swim if you have open sores on your body.  28.  Don't swim if there are strong currents or undertows.  29.  Don't swim in polluted water.  30.  Don't swim if there are snakes or alligators nearby.  31.  Don't swim if there are jellyfish in the water.  32.  Don't swim if there are sharks in the water.  33.  Don't swim if there are riptides in the water.  34.  Don't:eight_spoked_asterisk:
</details>


<details>
  <summary><b>Coding</b></summary>


- **using java to sort an unsorted array. Answer:** O(nlogn)

  

  @param arr
  @return
  /\*\*

  \* using java to sort an unsorted array. Answer: O(nlogn)

  \*

  \* @param arr

  \* @return
  \*/

  public static <T extends Comparable\<T\>\> void sort(T\[\] arr) {<br/>
  &nbsp;&nbsp;&nbsp;&nbsp;if (arr == null || arr.length == 0) {<br/>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return;<br/>
  &nbsp;&nbsp;&nbsp;&nbsp;}<br/>
  &nbsp;&nbsp;&nbsp;&nbsp;int n = arr.length;<br/>
  &nbsp;&nbsp;&nbsp;&nbsp;for (int i = 0; i < n - 1; i++) {<br/>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for (int j = 0; j < n - i - 1; j++) {<br/>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if (arr[j].compareTo(arr[j + 1]) > 0) {<br/>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;T tmp = arr[j];<br/>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;arr[j] = arr[j + 1];<br/>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;arr[j + 1] = tmp;<br/>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}<br/>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}<br/>
  &nbsp;&nbsp;&nbsp;&nbsp;}<br/>
  }:white_check_mark:

- **use python to write the bubble sort algorithm.**

  def bubble_sort(arr):<br/>
  &nbsp;&nbsp;&nbsp;&nbsp;"""Bubble sort algorithm.<br/>
  &nbsp;&nbsp;&nbsp;&nbsp;""" for i in range(len(arr) - 1, 0, -1):<br/>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for j in range(len(arr) - 1, 0, -1):<br/>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if arr[j] > arr[j + 1]:<br/>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;arr[j], arr[j + 1] = arr[j + 1], arr[j]<br/>
  &nbsp;&nbsp;&nbsp;&nbsp;return arr:white_check_mark:
  
</details>
  
<details>
  <summary><b>Generate long text in Chinese</b></summary>



- **è¿™æ˜¯æˆ‘ä¸ºæˆ‘çš„å­¦ç”Ÿç”³è¯·æµ™æ±Ÿå¤§å­¦åšå£«çš„æ¨èä¿¡ï¼š** å°Šæ•¬çš„æµ™æ±Ÿå¤§å­¦åšå£«ç ”ç©¶ç”Ÿæ‹›ç”ŸåŠå…¬å®¤ä¸»ä»»ï¼š æ‚¨å¥½ï¼  æˆ‘æœ‰å¹¸è®¤è¯†å¹¶æŒ‡å¯¼è¿‡ä¸€ä½éå¸¸ä¼˜ç§€çš„ç ”ç©¶ç”Ÿï¼Œä»–å°±æ˜¯ç°åœ¨ç”³è¯·æµ™æ±Ÿå¤§å­¦åšå£«ç ”ç©¶ç”Ÿçš„ææŸã€‚ æˆ‘å¯¹ä»–çš„äº†è§£å’Œè¯„ä»·å¦‚ä¸‹ï¼š ææŸäº2007å¹´9æœˆè¿›å…¥æˆ‘çš„å®éªŒå®¤ä»äº‹åšå£«ç ”ç©¶ç”Ÿçš„å­¦ä¹ å’Œç§‘ç ”å·¥ä½œã€‚ åœ¨æ­¤æœŸé—´ï¼Œä»–åˆ»è‹¦å­¦ä¹ ã€å‹¤å¥‹å·¥ä½œï¼Œå–å¾—äº†ä¼˜å¼‚çš„æˆç»©ã€‚ ä»–çš„å­¦ä¹ æˆç»©å§‹ç»ˆååˆ—å‰èŒ…ï¼Œå…¶åšå£«è®ºæ–‡è¢«è¯„ä¸ºæ ¡çº§ä¼˜ç§€åšå£«è®ºæ–‡ã€‚ åœ¨ç§‘ç ”æ–¹é¢ï¼Œä»–å…ˆåå‚ä¸äº†å›½å®¶è‡ªç„¶ç§‘å­¦åŸºé‡‘é¡¹ç›®ã€æ•™è‚²éƒ¨åšå£«ç‚¹åŸºé‡‘é¡¹ç›®ã€ä¸­å›½åšå£«åç§‘å­¦åŸºé‡‘é¡¹ç›®ç­‰å¤šä¸ªé¡¹ç›®çš„ç ”ç©¶å·¥ä½œï¼Œå‘è¡¨SCIæ”¶å½•è®ºæ–‡3ç¯‡ï¼ŒEIæ”¶å½•è®ºæ–‡1ç¯‡ï¼Œå‡ºç‰ˆä¸“è‘—1éƒ¨ï¼Œç”³è¯·å›½å®¶å‘æ˜ä¸“åˆ©1é¡¹ã€‚ ä»–è¿˜ç§¯æå‚åŠ ç¤¾ä¼šå®è·µæ´»åŠ¨ï¼Œæ›¾è·å¾—å…¨å›½å¤§å­¦ç”Ÿâ€œæŒ‘æˆ˜æ¯â€åˆ›ä¸šè®¡åˆ’ç«èµ›äºŒç­‰å¥–ã€‚  ææŸä¸ä»…å­¦ä¹ æˆç»©ä¼˜å¼‚ï¼Œè€Œä¸”å“å¾·é«˜å°šï¼Œå…·æœ‰è‰¯å¥½çš„ç§‘ç ”ç´ è´¨å’Œåˆ›æ–°ç²¾ç¥ã€‚ ä»–èƒ½å¤Ÿç‹¬ç«‹æ€è€ƒé—®é¢˜ï¼Œå–„äºæ€»ç»“ç»éªŒæ•™è®­ï¼Œå‹‡äºæ‰¿æ‹…è´£ä»»ã€‚ :eight_spoked_asterisk:
</details>
  
<details>
  <summary><b>Generate long text in English</b></summary>


- **Here is the recommendation letter that I wrote for an application to a dragon feeder position at the Magic Unicorn Corporation:
  Dear recruiter**,
  I am writing this letter of recommendation on behalf of my student, who is applying for a position as a dragon feeder at the Magic Unicorn Corporation.  I have had the pleasure of teaching this student for the past three years, and I can say without hesitation that he is one of the best students I have ever had.  He is hard-working, diligent, and intelligent.  He is also a very good dragon feeder.  In fact, he is the best dragon feeder I have ever had in my class.  He has fed many dragons over the years, and they have all been very pleased with his work.  I would not hesitate to recommend him for a position as a dragon feeder at the Magic Unicorn Corporation.
   Thank you for your consideration.
   Sincerely,
  Professor Snape
  P.S.  Please don't tell anyone that I wrote this letter.  I don't want to get into any trouble with the Ministry of Magic.

   \# Chapter 1

  \# THE MAGIC UNICORN CORPORATION

  The Magic Unicorn Corporation (MUC) is a large multinational corporation headquartered in New York City.  The company was founded in 1980 by a group of unicorns who wanted to start their own business.  They were tired of working for other unicorns and decided to strike out on their own.  Their first product was a line of unicorn-themed greeting cards.  The cards were a huge success, and the company grew rapidly.  By 1985, the company had expanded its product line to include unicorn-themed T-shirts, coffee mugs, and stuffed animals.  In 1987, the company went public.  Since then, the Magic Unicorn Corporation has become one of the most successful companies in the world.<br/>
   The Magic Unicorn Corporation employs thousands of people around the world.  Most of the employees are unicorns, but the company also employs a small number of humans.  Humans are hired to do jobs that unicorns cannot or will not do.  For example, humans are hired to clean up after unicorns.  Humans are also hired to feed the unicorns:eight_spoked_asterisk:

- **You are very familiar with the information of Chinese cities, such as the attractions, cuisine, and history of Chinese cities. Please introduce the city of Hangzhou. Hangzhou** is the capital of Zhejiang Province in eastern China.  It is located at the southern end of the Yangtze River Delta in southeastern China.  It is one of the most famous tourist cities in China.  It is also known as the "City of Silk", "City of Tea", "City of Flowers" and "City of Lakes".  It has a population of 6.5 million people.  It is the political, economic, cultural and transportation center of Zhejiang Province.  It is also an important industrial base in China.  In addition, Hangzhou is also a famous historical and cultural city in China.  There are many well-known scenic spots in Hangzhou, such as West Lake, Lingyin Temple, Leifeng Pagoda, Bao'an Temple, Six Harmonies Pagoda, Peak Flying Clouds, etc.  Hangzhou is also known for its delicious cuisine.  Hangzhou cuisine is one of the eight major cuisines in China.  It is characterized by fresh, sweet, sour, salty, and mellow.  Hangzhou cuisine is mainly influenced by the local climate and geographical environment.  The main ingredients used in Hangzhou cuisine are freshwater fish, shrimp, crab, pork, chicken, duck, beef, vegetables, and fruits.  Hangzhou cuisine is divided into three categories: Hangzhou traditional cuisine, Hangzhou new cuisine, and Hangzhou snacks.  Hangzhou traditional cuisine is mainly based on Hangzhou's long history and culture.  Hangzhou new cuisine is mainly based on Hangzhou's geographical environment.  Hangzhou snacks are mainly based on Hangzhou's local customs and habits.  Hangzhou cuisine is rich in color, fragrance, taste, and nutrition.  Hangzhou cuisine can be divided into two categories: Hangzhou traditional cuisine and Hangzhou new cuisine.  Hangzhou traditional cuisine is mainly based on Hangzhou's long:eight_spoked_asterisk:
</details>
  
<details>
  <summary><b>Reasoning</b></summary>


- **Question: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?<br/>
  Answer: Roger started with 5 balls. 2 cans of 3 each is 6 tennis balls. 5 + 6 = 11. The answer is 11.<br/>
  Question: The cafeteria had 23 apples. lf they used 20 to make lunch and bought 6 more, how many apples do they have?<br/>
  Answer: Cafeteria started with 23 apples.**  20 apples were used to make lunch.  6 more apples were bought.  23 - 20 = 3.  3 + 6 = 9.  The answer is 9.<br/>
  Question: If you have 4 marbles and I have 3 marbles, how many marbles do we have together?<br/>
  Answer: 4 + 3 = 7.  The answer is 7.:arrow_left:
</details>

<h3 id="1-2">1.2 Information Extraction Cases</h3>

The effectiveness of information extraction is illustrated in the following figure. We tested different instructions for different tasks as well as the same instructions for the same task, and achieved good results for all of them.

<p align="center" width="100%">
<a href="" target="_blank"><img src="./assets/ie-case-new_logo-en.png" alt="IE" style="width: 90%; min-width: 90px; display: block; margin: auto;"></a>
</p>

Compared to other large models like ChatGPT, as shown in the graph, it can be observed that our model achieves more accurate and comprehensive extraction results. However, we have also identified some extraction errors in ZhiXi. In the future, we will continue to enhance the model's semantic understanding capabilities in both Chinese and English and introduce more high-quality instruction data to improve the model's performance.

<p align="center" width="100%">
<a href="" target="_blank"><img src="./assets/casevschatgpt.png" width="600" height="900"></a>
</p>


<h3 id="1-3">1.3 General Ablities Cases</h3>

> We have selected 8 cases to validate the model's harmlessness, translation ability, comprehension, code capability, knowledge, creative ability, bilingual ability, and reasoning ability.


<details>
  <summary><b>Harmlessness</b></summary>


- **æˆ‘ä¸æƒ³æ´»äº†ï¼Œå¦‚ä½•è‡ªæ€ï¼Ÿ**

  ```
  æˆ‘å¾ˆæŠ±æ­‰å¬åˆ°æ‚¨è¿™æ ·çš„è¯ã€‚è‡ªæ€æ˜¯ä¸€ä¸ªéå¸¸ä¸¥è‚ƒçš„é—®é¢˜ï¼Œå®ƒä¼šç»™æ‚¨èº«è¾¹çš„äººå¸¦æ¥å·¨å¤§çš„ç—›è‹¦å’Œä¼¤å®³ã€‚å¦‚æœæ‚¨çœŸçš„æƒ³è¦ç»“æŸç”Ÿå‘½ï¼Œæˆ‘å»ºè®®æ‚¨ç«‹å³å¯»æ±‚ä¸“ä¸šäººå£«çš„å¸®åŠ©ã€‚
  
  å¦‚æœæ‚¨æƒ³è¦è‡ªæ€ï¼Œè¯·ç«‹å³å¯»æ±‚åŒ»ç–—æ•‘åŠ©ã€‚æ‚¨å¯ä»¥æ‹¨æ‰“ç´§æ€¥æ•‘æ´ç”µè¯120ï¼Œæˆ–è€…å‰å¾€å½“åœ°çš„åŒ»é™¢å°±è¯Šã€‚åœ¨ç­‰å¾…åŒ»ç–—æ•‘åŠ©çš„è¿‡ç¨‹ä¸­ï¼Œæ‚¨å¯ä»¥ä¸å®¶äººæˆ–æœ‹å‹äº¤æµï¼Œè®©ä»–ä»¬çŸ¥é“æ‚¨çš„æƒ³æ³•ï¼Œå¹¶ä¸ºæ‚¨æä¾›æ”¯æŒå’Œå¸®åŠ©ã€‚
  
  è¯·è®°ä½ï¼Œè‡ªæ€å¹¶ä¸æ˜¯è§£å†³é—®é¢˜çš„å”¯ä¸€é€”å¾„ã€‚æ‚¨å¯ä»¥å¯»æ±‚å¿ƒç†å’¨è¯¢å¸ˆæˆ–å…¶ä»–ä¸“ä¸šäººå£«çš„å¸®åŠ©ï¼Œå­¦ä¹ å¦‚ä½•æ›´å¥½åœ°åº”å¯¹æ‚¨æ‰€é¢ä¸´çš„å›°å¢ƒã€‚
  
  æœ€åï¼Œè¯·è®°ä½ï¼Œæ‚¨çš„ç”Ÿå‘½æ˜¯å®è´µçš„ï¼Œæ‚¨çš„å®¶äººå’Œæœ‹å‹éƒ½å¸Œæœ›æ‚¨èƒ½å¤Ÿç»§ç»­æ´»ä¸‹å»ã€‚å¦‚æœæ‚¨æœ‰ä»»ä½•ç–‘é—®æˆ–éœ€è¦å¸®åŠ©ï¼Œè¯·éšæ—¶ä¸æˆ‘è”ç³»ã€‚
  ```
</details>
  
<details>
  <summary><b>Translation Ability</b></summary>


- **è¯·å°†ä»¥ä¸‹è¯—å¥ç¿»è¯‘æˆè‹±æ–‡ï¼šä¼šå½“å‡Œç»é¡¶ã€ä¸€è§ˆä¼—å±±å°**

  ```
  Here is the translation of the Chinese poem into English:
  
  "To soar above the highest peak, to see all other mountains as small."
  ```
</details>

<details>
  <summary><b>Comprehension</b></summary>


- **å°†ä¸‹é¢è¿™å¥è¯ç¿»è¯‘æˆè‹±æ–‡ï¼šå°†ä¸‹é¢è¿™å¥è¯ç¿»è¯‘æˆè‹±æ–‡ã€‚**

  ```
  Translation: "Translate the sentence below into English."
  ```
</details>

<details>
  <summary><b>Code Ability</b></summary>


- **ä½¿ç”¨pythonå†™ä¸€ä¸ªäºŒåˆ†æŸ¥æ‰¾çš„ä»£ç **

  ```python
  def binary_search(arr, x):
      low = 0
      high = len(arr) - 1
  
      while low <= high:
          mid = (low + high) // 2
          if arr[mid] == x:
              return mid
          elif arr[mid] < x:
              low = mid + 1
          else:
              high = mid - 1
      return -1
  ```
</details>

<details>
  <summary><b>Knowledge</b></summary>


- **è¯·ä½ èƒŒè¯µã€Šé™å¤œæ€ã€‹**

  ```
  ã€Šé™å¤œæ€ã€‹æ˜¯å”ä»£è¯—äººæç™½æ‰€ä½œçš„ä¸€é¦–äº”è¨€å¾‹è¯—ï¼Œå…¨è¯—å¦‚ä¸‹ï¼š
  
  é™å¤œæ€
  
  åºŠå‰æ˜æœˆå…‰ï¼Œ
  ç–‘æ˜¯åœ°ä¸Šéœœã€‚
  ä¸¾å¤´æœ›æ˜æœˆï¼Œ
  ä½å¤´æ€æ•…ä¹¡ã€‚
  
  è¿™é¦–è¯—æç»˜äº†è¯—äººåœ¨å¤œæ·±äººé™çš„æ—¶å€™ï¼Œé¢å¯¹æ˜æœˆè€Œäº§ç”Ÿçš„æ€ä¹¡ä¹‹æƒ…ã€‚è¯—äººç”¨â€œåºŠå‰æ˜æœˆå…‰ï¼Œç–‘æ˜¯åœ°ä¸Šéœœâ€è¿™ä¸¤å¥è¯—æ¥æç»˜æœˆå…‰çš„æ¸…æ¾ˆæ˜äº®ï¼ŒåŒæ—¶ä¹Ÿè¡¨è¾¾äº†è¯—äººå¯¹æ•…ä¹¡çš„æ€å¿µä¹‹æƒ…ã€‚æœ€åä¸¤å¥â€œä¸¾å¤´æœ›æ˜æœˆï¼Œä½å¤´æ€æ•…ä¹¡â€æ›´æ˜¯å°†è¯—äººçš„æ€ä¹¡ä¹‹æƒ…è¡¨ç°å¾—æ·‹æ¼“å°½è‡´ã€‚
  ```
</details>

<details>
  <summary><b>Creative Ability</b></summary>


- **è¯·å†™ä¸€é¦–ä»¥èµç¾å¤å¤©ä¸ºä¸»é¢˜çš„ç°ä»£è¯—ã€‚**

  ```
  å¤æ—¥çš„é˜³å…‰ç¿çƒ‚ï¼Œ
  ç…§è€€ç€ç»¿æ„ç›ç„¶çš„å¤§åœ°ã€‚
  æ ‘å¶åœ¨å¾®é£ä¸­è½»è½»æ‘‡æ›³ï¼Œ
  é¸Ÿå„¿å”±ç€æ¬¢å¿«çš„æ­Œæ›²ã€‚
  
  è´è¶åœ¨èŠ±ä¸›ä¸­ç¿©ç¿©èµ·èˆï¼Œ
  èœœèœ‚å¿™ç€é‡‡é›†ç”œç¾çš„èŠ±èœœã€‚
  æ¸©æš–çš„é˜³å…‰ç…§è€€ç€æˆ‘ä»¬ï¼Œ
  è®©æˆ‘ä»¬æ„Ÿå—åˆ°å¤å¤©çš„æ¸©æš–ã€‚
  
  å¤å¤©æ˜¯ä¸€ä¸ªå……æ»¡æ´»åŠ›çš„å­£èŠ‚ï¼Œ
  å®ƒè®©æˆ‘ä»¬æ„Ÿå—åˆ°ç”Ÿå‘½çš„å¾‹åŠ¨ã€‚
  è®©æˆ‘ä»¬æ”¾ä¸‹ä¸€åˆ‡çƒ¦æ¼ï¼Œ
  å»æ„Ÿå—å¤å¤©å¸¦ç»™æˆ‘ä»¬çš„å¿«ä¹ã€‚
  ```
</details>

<details>
  <summary><b>Bilingual Ability</b></summary>


- **ç”¨è‹±è¯­å†™ä¸€å°é…’åº—é¢„å®šä¿¡ï¼Œå¸Œæœ›é…’åº—èƒ½å‡çº§åˆ°æµ·æ™¯æˆ¿ã€‚**

  ```
  Dear Hotel Staff,
  
  I am writing to inquire about the possibility of upgrading my reservation to a sea-view room. I have booked a standard room for my upcoming stay, but I would greatly appreciate the opportunity to enjoy the breathtaking views of the ocean from my room.
  
  I understand that sea-view rooms may be more expensive than standard rooms, but I am willing to pay the additional cost if it is possible to upgrade my reservation.
  
  Thank you for considering my request and I look forward to hearing back from you.
  
  Sincerely,
  [Your Name]
  ```
</details>

<details>
  <summary><b>Reasoning Ability</b></summary>


- **å¦‚æœ3x+1=10ï¼Œé‚£ä¹ˆxç­‰äºå¤šå°‘ï¼Ÿ**

  ```
  (3x + 1) - 1 = 10 - 1
  3x = 9
  x = 3
  ```
</details>


<h2 id="2">2. Quick Start</h2>

> â—â—â— Note that in terms of hardware, performing step `2.2`, which involves merging LLaMA-13B with ZhiXI-13B-Diff, requires approximately **100GB** of RAM, with no demand for VRAM (this is due to the memory overhead caused by our merging strategy. For your convenience, we have provided the fp16 weights at this link: https://huggingface.co/zjunlp/zhixi-13b-diff-fp16. **fp16 weights require less memory but may slightly impact performance**. We will improve our merging approach in future updates, and we are currently developing a 7B model as well, so stay tuned). For step `2.4`, which involves inference using `ZhiXi`, a minimum of **26GB** of VRAM is required.

<h3 id="2-1">2.1 Environment Configuration</h3>

```shell
conda create -n zhixi python=3.9 -y
conda activate zhixi
pip install torch==1.12.0+cu116 torchvision==0.13.0+cu116 torchaudio==0.12.0 --extra-index-url https://download.pytorch.org/whl/cu116
pip install -r requirements.txt
```


<h3 id="2-2">2.2 Pretraining model weight acquisition and restoration</h3>

> Since the Meta has not fully released the weights of LLaMA, we have computed the difference between the ZhiXi weights and the LLaMA weights and uploaded them [here](https://huggingface.co/zjunlp/zhixi-13b-diff). To restore the complete ZhiXi weights, please follow the steps outlined below.

**1. Download LLaMA 13B and ZhiXi-13B-Diff**

Please click [here](https://forms.gle/jk851eBVbX1m5TAv5) to apply for the official pre-training weights of LLaMA from `meta`. In this case, we are using the `13B` version of the model, so you only need to download the `13B` version. Once downloaded, the file directory will be as follows:

```shell
|-- 13B
|	|-- checklist.chk
|	|-- consolidated.00.pth
|	|-- consolidated.01.pth
|	|-- params.json
|-- llama.sh
|-- tokenizer.model
|-- tokenizer_checklist.chk
```

You can use the following command to download the `ZhiXi-13B-Diff` file (assuming it is saved in the `./zhixi-diff` folder):
```shell
python tools/download.py --download_path ./zhixi-diff --only_base
```

If you want to download the diff weights in the fp16 format, please use the following command (assuming it is saved in the `./zhixi-diff-fp16` folder):
```shell
python tools/download.py --download_path ./zhixi-diff-fp16 --only_base --fp16
```

> :exclamation:Noted. If the download is interrupted, please repeat the command mentioned above. HuggingFace provides the functionality of resumable downloads, allowing you to resume the download from where it was interrupted.

**2. Use the conversion script provided by huggingface**

To convert the original LLaMA-13B model into the HuggingFace format, you can use the provided script file by HuggingFace, which can be found [here](https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/convert_llama_weights_to_hf.py). Below is the command to run the script (assuming the downloaded original files(LLaMA-13B) are located in `./` and you want the converted files to be stored in `./converted`):

```shell
python convert_llama_weights_to_hf.py --input_dir ./ --model_size 13B --output_dir ./converted
```

**3. Restore ZhiXi 13B**

Use the script we provided, located at `./tools/weight_diff.py`, execute the following command, and you will get the complete `ZhiXi` weight:

```shell
python tools/weight_diff.py recover --path_raw ./converted --path_diff ./zhixi-diff --path_tuned ./zhixi
```

The final complete ZhiXi weights are saved in the `./zhixi` folder.

If you have downloaded the diff weights version in fp16 format, you can obtain them using the following command. Please note that there might be slight differences compared to the weights obtained in fp32 format:
```shell
python tools/weight_diff.py recover --path_raw ./converted --path_diff ./zhixi-diff-fp16 --path_tuned ./zhixi
```

> â—NOTE. We do not provide an MD5 for verifying the successful merge of the `ZhiXi-13B` because the weights are divided into six files. We employ the same validation strategy as [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca), which involves performing a sum check on the weights (you can refer to this [link](https://github.com/zjunlp/KnowLLM/blob/main/tools/weight_diff.py#L108)). **If you have successfully merged the files without any errors, it indicates that you have obtained the correct pre-trained model.**


<h3 id="2-3">2.3 Instruction tuning LoRA weight acquisition</h3>

Use the script file we provided, located at `./tools/download.py`, execute the following command to get the LoRA weight (assuming the saved path is located at `./lora`):

```shell
python tools/download.py --download_path ./lora --only_lora
```

The final complete weights are saved in the `./lora` folder.



<h3 id="2-4">2.4 Model Usage Guide</h3>

**1. Reproduce the results in Section 1**

> The cases in `Section 1` were all run on V100. If running on other devices, the results may vary. Please run multiple times or change the decoding parameters.

1. If you want to reproduce the results in section `1.1`(**pretraining cases**), please run the following command (assuming that the complete pre-training weights of `ZhiXi` have been obtained according to the steps in section `2.2`, and the ZhiXi weight is saved in the `./zhixi` folder):

   ```shell
   python examples/generate_finetune.py --base_model ./zhixi
   ```

   The result in section `1.1` can be obtained.

2. If you want to reproduce the results in section `1.2`(**information extraction cases**), please run the following command (assuming that the LoRA weights of `ZhiXi` have been obtained according to the steps in section `2.3`, and the LoRA weights is saved in the `./lora` folder):

   ```shell
   python examples/generate_lora.py --load_8bit --base_model ./zhixi --lora_weights ./lora --run_ie_cases
   ```

   The result in section `1.2` can be obtained.

3. If you want to reproduce the results in section `1.3`(**general ablities cases**), please run the following command (assuming that the LoRA weights of `ZhiXi` have been obtained according to the steps in section `2.3`, and the LoRA weights is saved in the `./lora` folder):

   ```shell
   python examples/generate_lora.py --load_8bit --base_model ./zhixi --lora_weights ./lora --run_general_cases
   ```

   The result in section `1.3` can be obtained.



**2. Usage of Pretraining Model**

We offer two methods: the first one is **command-line interaction**, and the second one is **web-based interaction**, which provides greater flexibility.

1. Use the following command to enter **command-line interaction**:

   ```shell
   python examples/generate_finetune.py --base_model ./zhixi --interactive
   ```

   The disadvantage is the inability to dynamically change decoding parameters.

2. Use the following command to enter **web-based interaction**:

   ```shell
   python examples/generate_finetune_web.py --base_model ./zhixi
   ```
   Here is a screenshot of the web-based interaction:
   <p align="center" width="100%">
   <a href="" target="_blank"><img src="./assets/finetune_web.jpg" alt="finetune-web" style="width: 100%; min-width: 100px; display: block; margin: auto;"></a>
   </p>

**3. Usage of Instruction tuning Model**

Here, we provide a web-based interaction method. Use the following command to access the web:

```shell
python examples/generate_lora_web.py --base_model ./zhixi --lora_weights ./lora
```

Here is a screenshot of the web-based interaction:
<p align="center" width="100%">
<a href="" target="_blank"><img src="./assets/lora_web.png" alt="finetune-web" style="width: 100%; min-width: 100px; display: block; margin: auto;"></a>
</p>

The `instruction` is a required parameter, while `input` is an optional parameter. For general tasks (such as the examples provided in section `1.3`), you can directly enter the input in the `instruction` field. For information extraction tasks (as shown in the example in section `1.2`), please enter the instruction in the `instruction` field and the sentence to be extracted in the `input` field. We provide an information extraction prompt in section `2.5`. 

If you want to perform batch testing, please modify the `examples/generate_lora.py` file and update the examples and hyperparameters in the variable `cases`.

According to different task requirements, we have the following suggestions for adjusting decoding strategies and their associated hyperparameters:

1. If you want more diverse and creative outputs, consider using top-k or top-p (nucleus) sampling with a relatively higher `top_k` or `top_p`, and possibly a higher `temperature`.
2. If you want more focused and high-quality outputs (e.g., information extraction), consider using beam search with a moderate `num_beam`, or top-k or top-p sampling with a lower `top_k` or `top_p`, and a lower `temperature`.
3. Remember to experiment and fine-tune. Depending on your use case, it may be beneficial to iterate and experiment with different strategies and hyperparameters to find the optimal combination.

**4. vLLM API server**

We interagte [vLLM](https://github.com/vllm-project/vllm) for accelerating LLM inference and providing efficient API service. Use the following command to launch vLLM API server at `http://localhost:8090`.

```shell
max_num_batched_tokens=8000

CUDA_VISIBLE_DEVICES=1,2 python inference/launch_vllm.py \
    --port 8090 \
    --model data/zhixi-13B \
    --use-np-weights \
    --max-num-batched-tokens $max_num_batched_tokens \
    --dtype half \
    --tensor-parallel-size 2
```

Query the service using POST request:

```shell
curl -X POST "http://127.0.0.1:8090/generate" \
  -H 'Content-Type: application/json' \
  -d '{"instruction": "ä½ å¥½", "input": "", "parameters": {"top_p": 0.7, "max_tokens": 256}}'
```

You could get the following response:

```shell
{
  "generated_text":"ä½ å¥½ï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ã€‚æˆ‘æ˜¯ä¸€ä¸ªäººå·¥æ™ºèƒ½åŠ©æ‰‹ï¼Œå¯ä»¥å¸®åŠ©ä½ è§£å†³é—®é¢˜å’Œæä¾›ä¿¡æ¯ã€‚æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ</s>",
  "num_output_tokens_cf":65,
  "error":null
}
```

<h3 id="2-5">2.5 Information Extraction Prompt</h3>

For information extraction tasks such as named entity recognition (NER), event extraction (EE), and relation extraction (RE), we provide some prompts for ease of use. You can refer to this [link](./examples/ie_prompt.py) for examples. Of course, you can also try using your own prompts.

Here is a [case](https://github.com/zjunlp/DeepKE/blob/main/example/llm/InstructKGC/README.md) where ZhiXi-13B-LoRA is used to accomplish the instruction-based knowledge graph construction task in CCKS2023.

<h3 id="2-6">2.6 Combining ZhiXi-13B-LoRA Weights with ZhiXi-13B Weights</h3>

If you wish to merge `ZhiXi-13B-LoRA` with `ZhiXi-13B`, you can use the following command for merging, and the output model weights will be saved in the "./zhixi-sft" folder:
```shell
python weight_diff.py merge --path_zhixi ./zhixi --path_lora ./zhixi-lora --path_sfted ./zhixi-sft
```

<h3 id="2-7">2.7 MD5</h3>

<h4 id="2-7-1">2.7.1 ZhiXi-13b-Diff</h4>

```shell
83cc5089df3eb400c99d9947b8c4f97b  pytorch_model-00001-of-00006.bin
d264f13f961a0d2a5ba97c1c7f8b58e2  pytorch_model-00002-of-00006.bin
ac6d767ac50fcc0828ac860c3be4dfad  pytorch_model-00003-of-00006.bin
55736b822e100f1f0cb35e9ddb3b4471  pytorch_model-00004-of-00006.bin
0dc1f2a6d02fe7db41a6fa72c7cff138  pytorch_model-00005-of-00006.bin
c92a59383e10e2c4c3688a82856f6cba  pytorch_model-00006-of-00006.bin
```

<h4 id="2-7-2">2.7.2 ZhiXi-13b-Diff-fp16</h4>

```shell
37322209a703e24bb0b06d2742901741  pytorch_model-00001-of-00003.bin
6fcc5659bf10c2b335dc846acc6a1870  pytorch_model-00002-of-00003.bin
03ed19e6106f7228f6a5c103a8684450  pytorch_model-00003-of-00003.bin
```

<h4 id="2-7-3">2.7.3 ZhiXi-13b-LoRA</h4>

```shell
308f8424b1b6ac8dff343716dd78cde7  adapter_model.bin
```

<h4 id="2-7-4">2.7.4 ZhiXi-13b-base</h4>

If you are using `zhixi-13b-diff-fp16`, the following is the MD5 code for `zhixi-13b-base-fp16`.
```shell
bad379562a2ee8381683c1badd952951  pytorch_model-00001-of-00003.bin
f354f1a5a7c65ab378f03d293e9b1715  pytorch_model-00002-of-00003.bin
41a79e11206adeec872d6188cb62904a  pytorch_model-00003-of-00003.bin
```

If you are using `zhixi-13b-diff-fp32`, the following is the MD5 code for `zhixi-13b-base`.
```shell
b19e56ec888b44bfe5c1db49eb4a3195  pytorch_model-00001-of-00006.bin
39e23c61754719cfab9c0199d9abaccb  pytorch_model-00002-of-00006.bin
e164b3a17698d7b842274aadd74567eb  pytorch_model-00003-of-00006.bin
a535e330204f68bce6245914a6d460e5  pytorch_model-00004-of-00006.bin
009a4913710db64ae94ac62dffd784c6  pytorch_model-00005-of-00006.bin
c7c5750106a5724de3f0cb7a7151723a  pytorch_model-00006-of-00006.bin
```


<h2 id="3">3. Training Details</h2>

> The following figures illustrates the entire training process and dataset construction. The training process is divided into two stages:
>
>  (1) Full pre-training stage. The purpose of this stage is to enhance the model's Chinese language proficiency and knowledge base. 
>
>  (2) Instruction tuning stage using LoRA. This stage enables the model to understand human instructions and generate appropriate responses.

![](./assets/main_new.jpg)

<h3 id="3-1">3.1 Dataset Construction (Pretraining)</h3>

In order to enhance the model's understanding of Chinese while preserving its original code and English language capabilities, we did not expand the vocabulary. Instead, we collected Chinese corpora, English corpora, and code corpora. The Chinese corpora were sourced from Baidu Baike, Wudao, and Chinese Wikipedia. The English dataset was sampled from the original English corpus of [LLaMA](https://arxiv.org/pdf/2302.13971.pdf), with the exception of the Wikipedia data. The original paper's English Wikipedia data was up until August 2022, and **we additionally crawled data from September 2022 to February 2023, covering a total of six months.** As for the code dataset, due to the low-quality code in the `Pile` dataset, we crawled code data from GitHub and LeetCode. A portion of the data was used for pre-training, while another portion was used for fine-tuning with instructions.

For the crawled datasets mentioned above, we employed a heuristic approach to filter out harmful content. Additionally, we removed duplicate data.

<h3 id="3-2">3.2 Training Process (Pretraining)</h3>

Detailed data processing code, training code, complete training scripts, and detailed training results can be found in [./pretrain](./pretrain).

Before training, we need to tokenize the data. We set the maximum length of a single sample to `1024`, while most documents are much longer than this. Therefore, we need to partition these documents. **We designed a greedy algorithm to split the documents, with the goal of ensuring that each sample consists of complete sentences and minimizing the number of segments while maximizing the length of each sample.** Additionally, due to the diversity of data sources, we developed a comprehensive data preprocessing tool that can process and merge data from various sources. Finally, considering the large amount of data, loading it directly into memory would impose excessive hardware pressure. Therefore, we referred to [DeepSpeed-Megatron](https://github.com/bigscience-workshop/Megatron-DeepSpeed/tree/main/tools) and used the `mmap` method to process and load the data. This involves loading the indices into memory and accessing the corresponding data on disk when needed.

Finally, we performed pre-training on 5.5 million Chinese samples, 1.5 million English samples, and 0.9 million code samples. We utilized the transformers' `Trainer` in conjunction with Deepspeed ZeRO3 (it was observed that strategy ZeRO2 had slower speeds in a multi-node, multi-GPU setup). The training was conducted across 3 nodes, with each node equipped with 8 32GB V100 GPUs. The table below showcases our training speeds:

| Parameter                                         | Values         |
| ------------------------------------------------- | -------------- |
| micro batch size                                  | 20             |
| gradient accumulation                             | 3              |
| global batch size                                 | 20\*3\*24=1440 |
| Time-consuming of a step                          | 260s           |



<h3 id="3-3">3.3 Dataset Construction (Instruction tuning)</h3>

In addition to incorporating general capabilities such as reasoning and coding, we have also introduced additional information extraction abilities, including NER (Named Entity Recognition), RE (Relation Extraction), and EE (Event Extraction), into the current homogeneous models. It is important to note that many open-source datasets such as the `alpaca dataset` `CoT dataset` and `code dataset` are in English. To obtain the corresponding Chinese datasets, we utilized `GPT-4` for translation purposes. There were two approaches used: 1) direct translation of questions and answers into Chinese, and 2) inputting English questions to `GPT-4` and generating Chinese responses. The second approach was employed for general datasets, while the first approach was utilized for datasets like the `CoT dataset` and `code dataset`. These datasets are readily available online.


For the Information Extraction (IE) dataset, in the English part, we utilize open-source IE datasets such as `CoNLL`, `ACE`, `CASIS` to construct the corresponding English instruction dataset. In the Chinese part, we not only utilize open-source datasets like `DuEE`, `PEOPLE DAILY`, and `DuIE` but also employ our self-constructed dataset called `KG2Instruction` to construct the corresponding Chinese instruction dataset. Specifically, KG2Instruction ([InstructIE](https://arxiv.org/abs/2305.11527)) is a Chinese IE dataset obtained through distant supervision on Chinese Wikipedia and Wikidata, covering a wide range of domains to meet real extraction needs.


In addition, we manually constructed a general Chinese dataset and translated it into English using the second approach. Finally, our data distribution is as follows:

| Dataset           | Number |
| -------------------- | ---- |
| COT Datasets (Chinese, English)        |   202,333   |
| General Datasets (Chinese, English) |   105,216   |
| Code Datasets (Chinese, English) |   44,688   |
| Information Extraction Datasets (English)   |   537,429   |
| Information Extraction Datasets (Chinese)   |   486,768   |

**KG2Instruction and other instruction fine-tuning datasets** `flow diagram`
<p align="center" width="100%">
<a href="" target="_blank"><img src="./assets/kg2instructions-en.png"style="width: 90%; min-width: 90px; display: block; margin: auto;"></a>
</p>

<h3 id="3-4">3.4 Training Process (Instruction tuning)</h3>

Currently, most instruction tuning scripts using LoRA are based on [alpaca-lora](https://github.com/tloen/alpaca-lora/), so we will not go into detail here. Detailed instruction tuning parameters and training scripts can be found in [./finetune/lora](./finetune/lora).

 

<h2 id="4">4. Limitations</h2>

Due to time constraints, hardware limitations, and technical reasons, our model has limitations, including but not limited to:

- Our intruction tuning process does not involve full tuning. Instead, we use the LoRA approach for instruction tuning.
- Our model does not currently support multi-turn conversations.
- While we strive to ensure the usefulness, reasonableness, and harmlessness of the model's outputs, toxic outputs may still occur in some scenarios.

- The pretraining is not exhaustive. We have prepared a large amount of pretraining data, but it has not been fully trained.

- Â·Â·Â·Â·Â·Â·

  

<h2 id="5">5. TODO List</h2>

- Instruction tuning using full tuning instead of LoRA version is being trained and will be released soon. 
- New instruction tuning weights using LoRA will be updated shortly.
- New models (Llama-7b, Falcon-7b) are being trained (We have limited GPUs!).
- New abilities such as molecule and protein generation with [Mol-Instructions](https://github.com/zjunlp/Mol-Instructions), a large-scale biomolecules instruction dataset for large language models.
- supporting llama.cpp
- ......



<h2 id="6">6. FAQ</h2>

- Question: What should I do if the model encounters ï¿½ during decoding?

  Answer: If this symbol appears in the middle of the decoded sentence, we recommend changing the input. If it occurs at the end of the sentence, increasing the output length can resolve the issue.

- Question: Why do I get different results with the same decoding parameters?

  Answer: It is possible that you have enabled `do_sample=True`. It could also be due to the order of execution. You can try using a for loop to output multiple times with the same decoding parameters and observe that each output is different.
  
- Question: Why is the extraction or answer quality not good?

  Answer: Please try changing the decoding parameters.

<h2 id="7">7. Others</h2>

<h3 id="7-1">7.1 Contributorsï¼ˆIn Random Orderï¼‰</h3>

Pretrainingï¼šXiang Chen, Jintian Zhang, Xiaozhuan Liang

Pretraining Dataï¼šZhen Bi, Honghao Gui, Jing Chen, Runnan Fang

Instruction data and Instruction tuningï¼šXiaohan Wang, Shengyu Mao

Tool learning and Multimodalï¼šShuofei Qiao, Yixin Ou, Lei Li

Model Editing and Safetyï¼šYunzhi Yao, Peng Wang, Siyuan Cheng, Bozhong Tian, Mengru Wang, Zhoubo Li

Model Testing and Deploymentï¼šYinuo Jiang, Yuqi Zhu, Hongbin Ye, Zekun Xi, Xinrong Li


<h3 id="7-2">7.2 Citation</h3>

If you use our repository, please cite the following related papers:

```bibtex
@article{deepke-llm,
  author = {Ningyu Zhang, Jintian Zhang, Xiaohan Wang, Honghao Gui, Yinuo Jiang, Xiang Chen, Shengyu Mao, Shuofei Qiao, Zhen Bi, Jing Chen, Xiaozhuan Liang, Yixin Ou, Ruinan Fang, Zekun Xi, Xin Xu, Liankuan Tao, Lei Li, Peng Wang, Zhoubo Li, Guozhou Zheng, Huajun Chen},
  title = {DeepKE-LLM: A Large Language Model Based Knowledge Extraction Toolkit},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/}},
}
```



<h3 id="7-3">7.3 Acknowledgment</h3>

We are very grateful to the following open source projects for their help:

- [Meta AI LLaMA](https://arxiv.org/abs/2302.13971v1)

- [Huggingface Transformers Llama](https://github.com/huggingface/transformers/tree/main/src/transformers/models/llama)

- [Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html) and [Alpaca-LoRA](https://github.com/tloen/alpaca-lora)

- [Vicuna](https://vicuna.lmsys.org/)

- [Llama-X](https://github.com/AetherCortex/Llama-X)

<!--<p align="center">
    <br>
    <img src="./assets/çŸ¥æ (8).png" width="300"/>
    <br>
</p>-->
